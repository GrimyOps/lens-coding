# Lens Coding Templates

This folder contains copy/paste templates for testing and using **Lens Coding** in any LLM environment (ChatGPT, Claude, Grok, Gemini, local models).

Lens Coding is a **constraint-first, evidence-anchored context discipline** designed to make AI failures more visible, traceable, and containable—without changing the model.

---

## Which template should I start with?

### Recommended adoption path
1. **Template 4 — Comparative Harness** (prove it works in your domain)
2. **Template 2 — Hidden Lens / Normal Chat** (best daily experience)
3. **Template 1 — Quick Start** (fastest onboarding)
4. **Template 3 — High-Stakes / Audit-Friendly** (highest rigor)

---

## Template Overview

### Template 1 — Quick Start
**Purpose:** Fast onboarding. Minimal structure that still captures the Lens Coding core.  
**Use when:** You want results quickly without heavy formatting.

### Template 2 — Hidden Lens / Normal Chat (Best Experience)
**Purpose:** Lens Coding runs behind the scenes while the user experience stays conversational.  
**Use when:** You want the best day-to-day usability after you trust the method.

### Template 3 — High-Stakes / Audit-Friendly
**Purpose:** Evidence IDs, constraint tracking, role execution, and blast-radius risk accounting.  
**Use when:** Healthcare, infra incidents, legal/compliance, finance—any environment requiring defensibility.

### Template 4 — Comparative Debug / Triage Harness (Recommended First)
**Purpose:** Run the same scenario twice (Standard vs Lens-Coded) and score differences.  
**Use when:** You want side-by-side results, repeatable testing, and objective evaluation.

---

## Notes
- Lens Coding does not guarantee correctness.
- Lens Coding improves *failure behavior* (early uncertainty, reduced overreach, better constraint adherence).
- Increased confidence does not necessarily equal increased accuracy—measure both.
